{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AcuOsr3FpXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0dcbd30-6fca-4c7f-a345-08fc3433c743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment and configuration\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from transformers import (\n",
        "    T5TokenizerFast,\n",
        "    TFT5ForConditionalGeneration,\n",
        "    create_optimizer,\n",
        ")\n",
        "\n",
        "import datasets as hf_datasets\n",
        "import evaluate as hf_evaluate\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = os.path.join(os.getcwd(), 'bitext-mortgage-loans-llm-chatbot-training-dataset.csv')\n",
        "SAVE_ROOT = os.path.join(os.getcwd(), 'saved_models')\n",
        "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
        "\n",
        "# Model and training hyperparameters\n",
        "MODEL_NAME = 'google/flan-t5-small'\n",
        "MAX_SOURCE_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 3e-4\n",
        "WARMUP_RATIO = 0.06\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "RUN_ID = time.strftime('HUFI_V1_FLAN_T5_%Y%m%d_%H%M%S')\n",
        "OUTPUT_DIR = os.path.join(SAVE_ROOT, RUN_ID)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f'Run ID: {RUN_ID}\\nSaving to: {OUTPUT_DIR}')\n"
      ],
      "metadata": {
        "id": "QDFv3W5OFwgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701e2b8e-e0d7-4a62-849e-cedf6c3c9747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run ID: HUFI_V1_FLAN_T5_20251013_141421\n",
            "Saving to: /content/saved_models/HUFI_V1_FLAN_T5_20251013_141421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and inspect dataset\n",
        "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
        "except Exception as e:\n",
        "    print(f\"Error reading CSV: {e}\")\n",
        "    # As a fallback, try reading with a different delimiter if applicable\n",
        "    # For example, if it might be tab-separated:\n",
        "    # df = pd.read_csv(DATA_PATH, sep='\\t', engine='python', on_bad_lines='skip')\n",
        "    # Or if it's a different common delimiter\n",
        "    # df = pd.read_csv(DATA_PATH, sep=';', engine='python', on_bad_lines='skip')\n",
        "    raise  # Re-raise the exception if reading still fails\n",
        "\n",
        "print(df.head(2))\n",
        "print('Columns:', df.columns.tolist())\n",
        "print('Shape:', df.shape)\n",
        "\n",
        "# Identify columns\n",
        "QUESTION_COL = 'instruction' if 'instruction' in df.columns else df.columns[1]\n",
        "ANSWER_COL = 'response' if 'response' in df.columns else df.columns[-1]\n",
        "\n",
        "# Clean basic\n",
        "for col in [QUESTION_COL, ANSWER_COL]:\n",
        "    df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "df = df.dropna(subset=[QUESTION_COL, ANSWER_COL])\n",
        "df = df.drop_duplicates(subset=[QUESTION_COL, ANSWER_COL])\n",
        "print('After cleaning:', df.shape)"
      ],
      "metadata": {
        "id": "wH4kB1aqF1UZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c09d3e-7a93-4ee8-82e0-63914954ebd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       system_prompt  \\\n",
            "0  You are an expert in customer support for mort...   \n",
            "1  You are an expert in customer support for mort...   \n",
            "\n",
            "                                         instruction          intent  \\\n",
            "0  could you help me to add a co-borrower to my m...  add_coborrower   \n",
            "1  I would like to add a co-borrower ot my auto l...  add_coborrower   \n",
            "\n",
            "             category    tags  \\\n",
            "0  LOAN_MODIFICATIONS    BILP   \n",
            "1  LOAN_MODIFICATIONS  BCILPZ   \n",
            "\n",
            "                                            response  \n",
            "0  I'm on it! I'm here to assist you with adding ...  \n",
            "1  Absolutely! I'm here to assist you in adding a...  \n",
            "Columns: ['system_prompt', 'instruction', 'intent', 'category', 'tags', 'response']\n",
            "Shape: (31038, 6)\n",
            "After cleaning: (31038, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified train/val/test split by intent if available\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "stratify_col = df['intent'] if 'intent' in df.columns else None\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED, shuffle=True, stratify=stratify_col)\n",
        "stratify_col_tv = train_df['intent'] if 'intent' in train_df.columns else None\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=SEED, shuffle=True, stratify=stratify_col_tv)\n",
        "\n",
        "print('Split sizes -> train:', len(train_df), 'val:', len(val_df), 'test:', len(test_df))\n"
      ],
      "metadata": {
        "id": "aJxHkndDF4XR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c8e470-7a85-4243-8ab2-2f14ab88c88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split sizes -> train: 25140 val: 2794 test: 3104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer and formatting\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "PREFIX = 'answer the question: '\n",
        "\n",
        "def format_example(question: str, answer: str):\n",
        "    return PREFIX + question, answer\n",
        "\n",
        "for i in range(2):\n",
        "    s, t = format_example(train_df.iloc[i][QUESTION_COL], train_df.iloc[i][ANSWER_COL])\n",
        "    print('SRC:', s[:100])\n",
        "    print('TGT:', t[:100])\n"
      ],
      "metadata": {
        "id": "Pd5kN7SJF8CG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "c3f6dcbfd61a4269a800ca1ef9df7e49",
            "1e0646d958b44b72b09efffcc33cc601",
            "3f1138c6996a40a99856da2d0458bdd4",
            "1dd96607bd8e432a93094a1c572adee0",
            "6169b93f6920476585326679fb183f3c",
            "88a2258ae4f34998873f40484fdb1fd8",
            "0edbe17dd4b14905b762ae94ef2e4e68",
            "1c40d615e9e44fbb81759c943ba36f51",
            "5274dca897184ffd96c668f89e7b221a",
            "d55d01daf8314cfa8957b6fd8cea2e59",
            "d9a446b207264f7dbcbc27ca543bfbc6",
            "ca32bfe519734370a7674bd981ed9aba",
            "cddde91a470e4a16aaae12a560975105",
            "cdd12c36e9fe46759ae9442bb0620ef3",
            "ac8dcee78b46415d86dc5b494d3bec05",
            "0e436df0d19643fdae8be1679d740119",
            "840d08be94784ee1b24449cc077b5cb2",
            "2ebee0d9bf7341599ba379dea9c02acc",
            "8082c2eceeca4565aadbdfd1bbf21985",
            "6b2a57a870a04609b28f9f224d135651",
            "1d5940c8d84e452faaeab337616d58ed",
            "bb9d66bb832b465dadcb5751192495ea",
            "e225b86bc5e9433392bbe812cc40d451",
            "d0d54bcbfe0244c0b1fb3a3e7c46d9b3",
            "e27bde2b550a42969bf35c98751a953d",
            "f5d1c4d955c94f73af7aa68e3b8f18ea",
            "a411732ae07b417abf177300dac105c9",
            "a87493b93b924bf284b38e44d93fb227",
            "e621dda5d3a04d89a63e340924c50cbd",
            "3e99bf04fbd94aecae49e7bf1275e196",
            "ceb4904c25b94adf9ee88e4a286f6242",
            "9ac435afc4304c9f873a3f5ecd2231d1",
            "00694b42d80c4b74aa8a4836acff781b",
            "bc05460abab34e409f5c4637928c8909",
            "4f7b45358e154a1da6bb15d2559ef858",
            "7a691e0d381d47628cec22b5b5436fe2",
            "650a6053206a45be87b2426f8a726b32",
            "03a3dedef05f4151867e7fe0e1b01124",
            "cb953dca367f4d9b97f93fc0bf3c7f02",
            "deee3dad06cd48d59d0bbe939abfa9e1",
            "2de06b0463ca4a71be44d4b62d319afc",
            "f6ae4abe38794276a5cf60ae372f05df",
            "99f4c037d28449e0920fa78f315dfd71",
            "3e761e1401104d358271c71118cf1871"
          ]
        },
        "outputId": "913afde9-02b9-47ec-d960-5188f86b9088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3f6dcbfd61a4269a800ca1ef9df7e49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca32bfe519734370a7674bd981ed9aba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e225b86bc5e9433392bbe812cc40d451"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc05460abab34e409f5c4637928c8909"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC: answer the question: help me checking the borrowing limit\n",
            "TGT: Assuredly! I'd be more than happy to assist you with checking the borrowing limit. The borrowing lim\n",
            "SRC: answer the question: I'd like to speak with an agent where could i do it\n",
            "TGT: I regret that any inconvenience you may have experienced. I understand that sometimes it's necessary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Hugging Face datasets\n",
        "\n",
        "def to_hf_dataset(frame: pd.DataFrame) -> hf_datasets.Dataset:\n",
        "    sources, targets = [], []\n",
        "    for _, row in frame.iterrows():\n",
        "        s, t = format_example(row[QUESTION_COL], row[ANSWER_COL])\n",
        "        sources.append(s)\n",
        "        targets.append(t)\n",
        "    return hf_datasets.Dataset.from_dict({'source': sources, 'target': targets})\n",
        "\n",
        "raw_train = to_hf_dataset(train_df)\n",
        "raw_val = to_hf_dataset(val_df)\n",
        "raw_test = to_hf_dataset(test_df)\n",
        "\n",
        "print(raw_train[0])\n"
      ],
      "metadata": {
        "id": "7b-sXd5QF_G4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4e1d2a-665e-4f6c-b6ea-b7b97b3ee0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'source': 'answer the question: help me checking the borrowing limit', 'target': 'Assuredly! I\\'d be more than happy to assist you with checking the borrowing limit. The borrowing limit is the maximum amount that you can borrow from us. It is based on various factors such as your credit history, income, and existing debts. To check your borrowing limit, you can:\\n\\n1. Log in to your account on our website/app.\\n2. Navigate to the \"Account\" or \"Profile\" section.\\n3. Look for the \"Borrowing Limit\" or \"Credit Limit\" option.\\n4. Click on it to view your current borrowing limit. \\n\\nIf you\\'re unable to find the option or have any specific questions regarding your borrowing limit, feel free to reach out to our customer support team. They can provide personalized assistance and guide you through the process.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize datasets\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    model_inputs = tokenizer(\n",
        "        batch['source'],\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='np',\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            batch['target'],\n",
        "            max_length=MAX_TARGET_LENGTH,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='np',\n",
        "        )\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "train_tokenized = raw_train.map(tokenize_function, batched=True, remove_columns=['source','target'])\n",
        "val_tokenized = raw_val.map(tokenize_function, batched=True, remove_columns=['source','target'])\n",
        "test_tokenized = raw_test.map(tokenize_function, batched=True, remove_columns=['source','target'])\n",
        "\n",
        "for ds in [train_tokenized, val_tokenized, test_tokenized]:\n",
        "    ds.set_format(type='numpy')\n"
      ],
      "metadata": {
        "id": "MnkZIiX9GDb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "5a7a2a02e99e4939b8dc63a8d973a15d",
            "282ea8cbe25140c2a7009b3deb636f3e",
            "ef4d20cc23274745bd5d4ceb1cfc3d32",
            "84295f7635dc4c61aa71ff74b153545a",
            "3b848cdf1ebc41998ea8622089e0de8e",
            "104a673ab7dd4a5abf623b62fa36fcbf",
            "51cdcba6f9ee44f9a0439637ff6c487b",
            "22ed9ca7099245ba87b6721c7bd0d899",
            "27df5d3019a3483e8220d4dd28d57839",
            "6aed7556e2e24d07a66fa5e78b53e6ec",
            "abc57cf4eb5548c1aed19a8e21179489",
            "4f9561336f9541a5b324babd5c0ba4a2",
            "decd08f3dd88463d98a344567821911b",
            "5f9dd805cd1c48eca99c22625404a96e",
            "ce57645db0344481806729fb5bd835ef",
            "d987dbcc6855440b8a322c5e43a25383",
            "e24e88dd6920413d8c7f2ecbef321452",
            "29ca48a7274342b7a81adc2c545be914",
            "baef8cc3fd9e45ff9aaa9d91e3966c26",
            "bc67d8ef1d644cc9aea464ca5032dfa2",
            "e72a63b646c74143b9bd63df1660b86c",
            "7c316937782846f29e69a82d4b25d56e",
            "a34d6d015ffc4db8a3253528c390354e",
            "7863286d07514d9099eb82dfc8aa8901",
            "8a51b7e8f5324a08a69bb4949a48ed22",
            "0e2e0958c3094d6a80d18aaeba8595ff",
            "8ac11aee5fa740c08212a500b18c3c94",
            "f73ed1801bc14f5b949e9407274a2cb6",
            "2daadf6a60cc4e94b36fe0f8b4ceb89f",
            "12ceaa3a844f496f9a82bc0475909b5e",
            "6ba4a6d7181942e089b6632f9451003c",
            "37bc80a88a14464dbd5fef1f8db37c30",
            "fe0e09de6837457190836bd33eeacfdc"
          ]
        },
        "outputId": "976f9fc0-9369-4317-fa2b-91a56b7f8e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25140 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a7a2a02e99e4939b8dc63a8d973a15d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2794 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f9561336f9541a5b324babd5c0ba4a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3104 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a34d6d015ffc4db8a3253528c390354e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.data input pipelines\n",
        "\n",
        "def to_tf_dataset(tokenized: hf_datasets.Dataset, batch_size: int) -> tf.data.Dataset:\n",
        "    feats = {\n",
        "        'input_ids': tokenized['input_ids'],\n",
        "        'attention_mask': tokenized['attention_mask'],\n",
        "        'labels': tokenized['labels'],\n",
        "    }\n",
        "    def gen():\n",
        "        for i in range(len(tokenized)):\n",
        "            yield {k: feats[k][i] for k in feats}\n",
        "    sig = {\n",
        "        'input_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "        'attention_mask': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "        'labels': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "    }\n",
        "    return tf.data.Dataset.from_generator(gen, output_signature=sig).shuffle(1024, seed=SEED).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = to_tf_dataset(train_tokenized, BATCH_SIZE)\n",
        "val_ds = to_tf_dataset(val_tokenized, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "-Xtre629GHmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and training loop\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(MODEL_NAME, from_pt=True)\n",
        "\n",
        "num_train_steps = math.ceil(len(train_tokenized) / BATCH_SIZE) * EPOCHS\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_RATIO)\n",
        "\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=LEARNING_RATE,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Change OUTPUT_DIR to a path in Google Drive\n",
        "OUTPUT_DIR = os.path.join(SAVE_ROOT, RUN_ID)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f'Run ID: {RUN_ID}\\nSaving to: {OUTPUT_DIR}')\n",
        "\n",
        "ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(OUTPUT_DIR, 'ckpt'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[ckpt_cb, es_cb],\n",
        ")\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print('Saved to', OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "-UAWn6WCGKXf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415,
          "referenced_widgets": [
            "e2c6dc06ec0447a19dc01cc244f77f1d",
            "c381649683f64a6a95b1bfc54eecdcc4",
            "7c27356d5d9b4463a42a8696e708c501",
            "eed2e62f05fd452c86cfa9d4b4d9996f",
            "0ef5412cea594184ae0de0a8a4026dcd",
            "999bb35ddb3a4c3390e67281a53a57c7",
            "902bcc6e1fe64c6dbdc018601aa1dfed",
            "ae172a8ff75343af841f2c51c24432c6",
            "aff4672906dd4333a4400481f2aecaff",
            "640b94445c194163bdf811130edcf34a",
            "e46aebffe07c4bc3a12ec9188fcc9e7f",
            "8680509589a14f099c5abecb24f811c8",
            "1a200faaa1a7489ca77e7ec62d8feab9",
            "26a93157cc684521afa605244a37e5cb",
            "a8a4bd63844941519bbb2cb773700bed",
            "d45a6abf408c433595ebb698dfc3b517",
            "2df6055f21284b46a20596d08e44cba5",
            "fe76015db5a04c7c88ac9702c42ec8b4",
            "69cfbd3050c349f8ab705b1b50bc4e64",
            "28abdd9b548d4a81ba6255863a01beda",
            "b2bb4372cbd54d9db123d13249a7d748",
            "17e9e070511945e29d7c5a215f7b7f0f"
          ]
        },
        "outputId": "cd781956-7780-4d21-d8ad-8f164aa73d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2c6dc06ec0447a19dc01cc244f77f1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8680509589a14f099c5abecb24f811c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "/usr/local/lib/python3.12/dist-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
            "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run ID: HUFI_V1_FLAN_T5_20251013_141421\n",
            "Saving to: /content/saved_models/HUFI_V1_FLAN_T5_20251013_141421\n",
            "Epoch 1/3\n",
            "3143/3143 [==============================] - 980s 302ms/step - loss: 1.2528 - val_loss: 0.7844\n",
            "Epoch 2/3\n",
            "3143/3143 [==============================] - 943s 299ms/step - loss: 0.8518 - val_loss: 0.7089\n",
            "Epoch 3/3\n",
            "3143/3143 [==============================] - 946s 301ms/step - loss: 0.7819 - val_loss: 0.6828\n",
            "Saved to /content/saved_models/HUFI_V1_FLAN_T5_20251013_141421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "nXkUcNpAGPfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16ccc27-c098-49c1-c971-370c927b568d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "LVIb5JYIGQNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac4dad7-a37a-41cf-a364-fc3c8e833c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9d396c59a444e2d2a7b4cf95410fdea8b5e813f17195b08d5269cebcbbdef27b\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: BLEU, ROUGE-L, perplexity\n",
        "\n",
        "bleu = hf_evaluate.load('sacrebleu')\n",
        "rouge = hf_evaluate.load('rouge')\n",
        "\n",
        "def generate_answers(questions, max_new_tokens=64):\n",
        "    inputs = tokenizer(['answer the question: ' + q for q in questions], return_tensors='tf', padding=True, truncation=True, max_length=MAX_SOURCE_LENGTH)\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "sample_size = min(256, len(val_df))\n",
        "sample_questions = val_df[QUESTION_COL].tolist()[:sample_size]\n",
        "sample_refs = [[a] for a in val_df[ANSWER_COL].tolist()[:sample_size]]\n",
        "\n",
        "preds = generate_answers(sample_questions)\n",
        "\n",
        "bleu_res = bleu.compute(predictions=preds, references=sample_refs)\n",
        "rouge_res = rouge.compute(predictions=preds, references=[r[0] for r in sample_refs])\n",
        "\n",
        "val_loss = model.evaluate(val_ds, return_dict=True)['loss']\n",
        "perplexity = math.exp(val_loss) if val_loss < 20 else float('inf')\n",
        "\n",
        "print('BLEU:', bleu_res)\n",
        "print('ROUGE-L:', rouge_res.get('rougeL'))\n",
        "print('Val loss:', val_loss, 'Perplexity:', perplexity)\n"
      ],
      "metadata": {
        "id": "lBP3Q3Nsi3sW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193,
          "referenced_widgets": [
            "7dceee26511e4c2d979037e259a4c985",
            "f471822ead8b40b39af4ba2e01cd75df",
            "4d4814c7cdd5466196d665c8cacd0a3a",
            "8486cad59308477e886f083aab676f0e",
            "79b3c54f1a4d4b53b52f25d3aacb437e",
            "a1f9e5930e02498c89a286bd3019c359",
            "bfca933f9581444f890b1bd0a73d1182",
            "b287ecb6240a466fa960069c0810b3c2",
            "2127c6e66c5340e1a41d43a432841015",
            "6b6a4e0f05a142829f688b2731217bdb",
            "a2c24bb3b90947eab013b70b1ce6c2cf",
            "23f029e63f734a338ea9e16580626047",
            "54100687e196457da5f5c1309c9cbade",
            "47a8f2f2b00044aca8321e0210934b37",
            "532edb018f5e4e37828a4f79b6bc6c04",
            "4e22eea7e89141e4a7fb5cb398450625",
            "a6407a08262143c28352aa50218d7d72",
            "98841b8d7ce5470d99a350f5e1e89afe",
            "1bb53ab988fc41caac962d2f5c4f201d",
            "00b3a4dbd12c41478dd27190907f6123",
            "98b05f7c1c6a4b5c9b9906c490598777",
            "d227217af3ad433a852eb2b807029abc"
          ]
        },
        "outputId": "768fbca0-d33d-44fb-d493-d27242bd7de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dceee26511e4c2d979037e259a4c985"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23f029e63f734a338ea9e16580626047"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "350/350 [==============================] - 39s 107ms/step - loss: 0.6820\n",
            "BLEU: {'score': 2.8819816181639233, 'counts': [10713, 6943, 5027, 3722], 'totals': [14150, 13894, 13638, 13382], 'precisions': [75.71024734982332, 49.97121059450122, 36.86024343745417, 27.81348079509789], 'bp': 0.06494103216020367, 'sys_len': 14150, 'ref_len': 52840}\n",
            "ROUGE-L: 0.28295411427809747\n",
            "Val loss: 0.6819899082183838 Perplexity: 1.977809478361478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qualitative test: predictions vs references on a small sample\n",
        "\n",
        "NUM_EXAMPLES = 10\n",
        "MAX_NEW_TOKENS = 64\n",
        "\n",
        "sample_questions = val_df[QUESTION_COL].tolist()[:NUM_EXAMPLES]\n",
        "sample_refs = val_df[ANSWER_COL].tolist()[:NUM_EXAMPLES]\n",
        "\n",
        "def generate_answers_list(questions, max_new_tokens=64):\n",
        "    inputs = tokenizer(\n",
        "        ['answer the question: ' + q for q in questions],\n",
        "        return_tensors='tf',\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "    )\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "preds = generate_answers_list(sample_questions, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "for i, (q, ref, pred) in enumerate(zip(sample_questions, sample_refs, preds), 1):\n",
        "    print(f'--- Example {i} ---')\n",
        "    print('Q:', q)\n",
        "    print('REF:', ref[:400])\n",
        "    print('PRED:', pred[:400])\n",
        "    print()\n",
        "\n",
        "print(f'Shown {NUM_EXAMPLES} qualitative examples.')\n"
      ],
      "metadata": {
        "id": "R7wSQikdGYAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5726993-95b2-4d3f-b7fb-6d40f3db5f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Example 1 ---\n",
            "Q: I got to modify my preferred account number, where can I do it?\n",
            "REF: Certainly, I'm here to help you with modifying your preferred account number. Here's what you need to do:\n",
            "\n",
            "1. Log in to your account on our website or mobile app.\n",
            "2. Navigate to the \"{{Account Settings}}\" or \"{{Profile}}\" section.\n",
            "3. Look for the option to update your account information.\n",
            "4. You should find a field specifically for your preferred account number.\n",
            "5. Enter the new account number you\n",
            "PRED: Certainly, I can assist you with modifying your preferred account number. Here's what you need to do: 1. Log in to your account on our website or mobile app. 2. Navigate to the \"Account Settings\" or \"Profile\" section. 3. Look for the option to manage your preferred account number\n",
            "\n",
            "--- Example 2 ---\n",
            "Q: can ya help me to see my fucking home loan details\n",
            "REF: We're accountable for, and we apologize for any frustration caused by the language used in your previous message. I'd be happy to assist you with accessing your home loan details. Here's what you need to do:\n",
            "\n",
            "1. Go to the website or app of your home loan provider.\n",
            "2. Log in to your account using your registered email address and password.\n",
            "3. Look for the section or tab related to your loan.\n",
            "4. Cli\n",
            "PRED: I genuinely apologize for any frustration or inconvenience you may have experienced while trying to access the details of your home loan. I'm here to assist you with that. To view your home loan details, please follow these steps: 1. Log in to your account on our website or mobile app. 2. Navigate to the\n",
            "\n",
            "--- Example 3 ---\n",
            "Q: I need to pay off my fucking student loan faster, could you help me to make a extra mortgage payment?\n",
            "REF: I genuinely apologize for the offensive language used in your instruction, and I understand your urgency in wanting to pay off your student loan faster. I'm here to assist you in making an additional payment towards your student loan. \n",
            "\n",
            "To make an extra payment and accelerate your loan payoff, you can follow these steps:\n",
            "\n",
            "1. Contact our customer support team at {{Customer Support Phone Number}} or\n",
            "PRED: I genuinely apologize for any frustration or inconvenience you may have experienced with your student loan. I understand your goal of paying it off faster and I'm here to assist you in making an extra payment. Here's what you need to do: 1. Contact our customer service team at Customer Support Phone Number\n",
            "\n",
            "--- Example 4 ---\n",
            "Q: I want to add a co-borrower to my loan, can you help me?\n",
            "REF: I'll get right on it! I'm here to assist you with adding a co-borrower to your loan. Adding a co-borrower can have several benefits, such as potentially increasing your chances of loan approval or securing a better interest rate. Here's what you need to do:\n",
            "\n",
            "1. Gather the necessary information: Make sure you have all the required details of the co-borrower, such as their full name, contact informa\n",
            "PRED: I'll make it happen! I'm here to assist you with adding a co-borrower to your loan. Adding a co-borrower can have several benefits, such as increasing your chances of loan approval or obtaining a better interest rate. Here's what you need\n",
            "\n",
            "--- Example 5 ---\n",
            "Q: id lie to estimate my fucking loan payment can ya help me\n",
            "REF: I apologize if there was any offensive language used in your previous message. I'm here to help you estimate your loan payment. Understanding your loan payment is essential for proper financial planning. Let's calculate your loan payment together. Could you please provide me with the following information:\n",
            "\n",
            "1. Loan amount: The total amount of money you're borrowing.\n",
            "2. Interest rate: The annual in\n",
            "PRED: I genuinely apologize for any frustration caused by the language used in your message. I understand that you would like assistance in estimating your loan payment. I'm here to help! Calculating your loan payment is an important step in managing your finances. To estimate your loan payment, we'll need a few\n",
            "\n",
            "--- Example 6 ---\n",
            "Q: I'd like to see your fucking fee  for late payment, how can I do it?\n",
            "REF: I genuinely apologize for any frustration or inconvenience you may have experienced with late payments. I can definitely assist you with understanding our fee for late payment. Here's what you need to know:\n",
            "\n",
            "Late Payment Fee Policy:\n",
            "- Our late payment fee is {{Late Payment Fee Amount}}.\n",
            "- The fee is charged when the payment is not received by the due date specified in your billing statement.\n",
            "- To \n",
            "PRED: I genuinely apologize for any frustration caused by the language used in your message. I understand that you would like to know more about our late payment fees. I'm here to assist you with that. To view our late payment fees, please follow these steps: 1. Visit our website at Company Website\n",
            "\n",
            "--- Example 7 ---\n",
            "Q: I need to make a fuckkng additional mortgage payment, can I get some help?\n",
            "REF: I genuinely apologize for any frustration you may have experienced while attempting to make an additional mortgage payment. I'm here to help you with that. Let's get this sorted out. To make an additional mortgage payment, follow these steps:\n",
            "\n",
            "1. Log in to your online banking account or visit our website at {{Company Website URL}}.\n",
            "2. Navigate to the \"Payments\" or \"Transfer\" section.\n",
            "3. Select the\n",
            "PRED: I genuinely apologize for the frustration caused by the language used in your message. I understand that you need assistance with making an additional mortgage payment. I'm here to help you with that. Here's what you need to do: 1. Log in to your account on our website or mobile app. 2. Navigate\n",
            "\n",
            "--- Example 8 ---\n",
            "Q: I would like to lock my fucking interest, could you help mew?\n",
            "REF: I sincerely apologize for any inconvenience or frustration you're experiencing. I assure you that I'm here to assist you with locking your interest rate. \n",
            "\n",
            "To proceed with locking your interest rate, please follow these steps:\n",
            "\n",
            "1. Contact our Customer Service department at {{Customer Service Phone Number}} or {{Customer Service Email Address}}. They will guide you through the process and provide y\n",
            "PRED: I genuinely apologize for any frustration or inconvenience you may have experienced with your interest rate. I'm here to assist you with locking your interest rate. To proceed with locking your interest rate, please follow these steps: 1. Contact our Mortgage Department at Customer Support Phone Number\n",
            "\n",
            "--- Example 9 ---\n",
            "Q: how can I see the status of my application?\n",
            "REF: For sure! I'll be happy to help you check the status of your application. To view the status of your application, you can follow these simple steps:\n",
            "\n",
            "1. Log in to your account on our website.\n",
            "2. Navigate to the \"Applications\" or \"My Applications\" section.\n",
            "3. Look for the specific application you want to check the status of.\n",
            "4. Click on the application to view its details.\n",
            "5. You should be able to \n",
            "PRED: I'll take care of it! I'm here to assist you with checking the status of your application. To check the status, you can follow these steps: 1. Log in to your account on our website or mobile app. 2. Navigate to the \"Applications\" or \"My Applications\" section\n",
            "\n",
            "--- Example 10 ---\n",
            "Q: I'd like to know about your late payment fees, whree can I find more information?\n",
            "REF: Sure, I can help you with that! Late payment fees are an important aspect to consider when it comes to managing your accounts. To find more information about our late payment fees, you can take the following steps:\n",
            "\n",
            "1. Visit our official website at {{Company Website URL}}.\n",
            "2. Navigate to the \"Support\" or \"Help\" section, usually located in the top or bottom menu.\n",
            "3. Look for the \"Payments,\" \"Billin\n",
            "PRED: I appreciate your interest in learning more about our late payment fees. I'm here to assist you with that. To find more information about our late payment fees, you can follow these steps: 1. Visit our website at Company Website URL. 2. Navigate to the \"Billing\" or\n",
            "\n",
            "Shown 10 qualitative examples.\n"
          ]
        }
      ]
    }
  ]
}
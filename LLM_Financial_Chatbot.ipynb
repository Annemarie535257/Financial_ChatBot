{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGnYbgZMVxhALzilXxyjGM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annemarie535257/Financial_ChatBot/blob/main/LLM_Financial_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AcuOsr3FpXj"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment and configuration\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from transformers import (\n",
        "    T5TokenizerFast,\n",
        "    TFT5ForConditionalGeneration,\n",
        "    create_optimizer,\n",
        ")\n",
        "\n",
        "import datasets as hf_datasets\n",
        "import evaluate as hf_evaluate\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = os.path.join(os.getcwd(), 'bitext-mortgage-loans-llm-chatbot-training-dataset.csv')\n",
        "SAVE_ROOT = os.path.join(os.getcwd(), 'saved_models')\n",
        "os.makedirs(SAVE_ROOT, exist_ok=True)\n",
        "\n",
        "# Model and training hyperparameters\n",
        "MODEL_NAME = 'google/flan-t5-small'\n",
        "MAX_SOURCE_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "WARMUP_RATIO = 0.06\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "RUN_ID = time.strftime('HUFI_V1_FLAN_T5_%Y%m%d_%H%M%S')\n",
        "OUTPUT_DIR = os.path.join(SAVE_ROOT, RUN_ID)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f'Run ID: {RUN_ID}\\nSaving to: {OUTPUT_DIR}')\n"
      ],
      "metadata": {
        "id": "QDFv3W5OFwgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and inspect dataset\n",
        "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}\"\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(df.head(2))\n",
        "print('Columns:', df.columns.tolist())\n",
        "print('Shape:', df.shape)\n",
        "\n",
        "# Identify columns\n",
        "QUESTION_COL = 'instruction' if 'instruction' in df.columns else df.columns[1]\n",
        "ANSWER_COL = 'response' if 'response' in df.columns else df.columns[-1]\n",
        "\n",
        "# Clean basic\n",
        "for col in [QUESTION_COL, ANSWER_COL]:\n",
        "    df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "df = df.dropna(subset=[QUESTION_COL, ANSWER_COL])\n",
        "df = df.drop_duplicates(subset=[QUESTION_COL, ANSWER_COL])\n",
        "print('After cleaning:', df.shape)\n"
      ],
      "metadata": {
        "id": "wH4kB1aqF1UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified train/val/test split by intent if available\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "stratify_col = df['intent'] if 'intent' in df.columns else None\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED, shuffle=True, stratify=stratify_col)\n",
        "stratify_col_tv = train_df['intent'] if 'intent' in train_df.columns else None\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=SEED, shuffle=True, stratify=stratify_col_tv)\n",
        "\n",
        "print('Split sizes -> train:', len(train_df), 'val:', len(val_df), 'test:', len(test_df))\n"
      ],
      "metadata": {
        "id": "aJxHkndDF4XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer and formatting\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "PREFIX = 'answer the question: '\n",
        "\n",
        "def format_example(question: str, answer: str):\n",
        "    return PREFIX + question, answer\n",
        "\n",
        "for i in range(2):\n",
        "    s, t = format_example(train_df.iloc[i][QUESTION_COL], train_df.iloc[i][ANSWER_COL])\n",
        "    print('SRC:', s[:100])\n",
        "    print('TGT:', t[:100])\n"
      ],
      "metadata": {
        "id": "Pd5kN7SJF8CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Hugging Face datasets\n",
        "\n",
        "def to_hf_dataset(frame: pd.DataFrame) -> hf_datasets.Dataset:\n",
        "    sources, targets = [], []\n",
        "    for _, row in frame.iterrows():\n",
        "        s, t = format_example(row[QUESTION_COL], row[ANSWER_COL])\n",
        "        sources.append(s)\n",
        "        targets.append(t)\n",
        "    return hf_datasets.Dataset.from_dict({'source': sources, 'target': targets})\n",
        "\n",
        "raw_train = to_hf_dataset(train_df)\n",
        "raw_val = to_hf_dataset(val_df)\n",
        "raw_test = to_hf_dataset(test_df)\n",
        "\n",
        "print(raw_train[0])\n"
      ],
      "metadata": {
        "id": "7b-sXd5QF_G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize datasets\n",
        "\n",
        "def tokenize_function(batch):\n",
        "    model_inputs = tokenizer(\n",
        "        batch['source'],\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='np',\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            batch['target'],\n",
        "            max_length=MAX_TARGET_LENGTH,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='np',\n",
        "        )\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "train_tokenized = raw_train.map(tokenize_function, batched=True, remove_columns=['source','target'])\n",
        "val_tokenized = raw_val.map(tokenize_function, batched=True, remove_columns=['source','target'])\n",
        "test_tokenized = raw_test.map(tokenize_function, batched=True, remove_columns=['source','target'])\n",
        "\n",
        "for ds in [train_tokenized, val_tokenized, test_tokenized]:\n",
        "    ds.set_format(type='numpy')\n"
      ],
      "metadata": {
        "id": "MnkZIiX9GDb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.data input pipelines\n",
        "\n",
        "def to_tf_dataset(tokenized: hf_datasets.Dataset, batch_size: int) -> tf.data.Dataset:\n",
        "    feats = {\n",
        "        'input_ids': tokenized['input_ids'],\n",
        "        'attention_mask': tokenized['attention_mask'],\n",
        "        'labels': tokenized['labels'],\n",
        "    }\n",
        "    def gen():\n",
        "        for i in range(len(tokenized)):\n",
        "            yield {k: feats[k][i] for k in feats}\n",
        "    sig = {\n",
        "        'input_ids': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "        'attention_mask': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "        'labels': tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
        "    }\n",
        "    return tf.data.Dataset.from_generator(gen, output_signature=sig).shuffle(1024, seed=SEED).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = to_tf_dataset(train_tokenized, BATCH_SIZE)\n",
        "val_ds = to_tf_dataset(val_tokenized, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "-Xtre629GHmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and training loop\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = TFT5ForConditionalGeneration.from_pretrained(MODEL_NAME, from_pt=True)\n",
        "\n",
        "num_train_steps = math.ceil(len(train_tokenized) / BATCH_SIZE) * EPOCHS\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_RATIO)\n",
        "\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=LEARNING_RATE,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=WEIGHT_DECAY,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Change OUTPUT_DIR to a path in Google Drive\n",
        "OUTPUT_DIR = os.path.join('/content/drive/MyDrive', 'saved_models', RUN_ID)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f'Run ID: {RUN_ID}\\nSaving to: {OUTPUT_DIR}')\n",
        "\n",
        "ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(OUTPUT_DIR, 'ckpt'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[ckpt_cb, es_cb],\n",
        ")\n",
        "\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print('Saved to', OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "-UAWn6WCGKXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "nXkUcNpAGPfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "id": "LVIb5JYIGQNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation: BLEU, ROUGE-L, perplexity\n",
        "\n",
        "bleu = hf_evaluate.load('sacrebleu')\n",
        "rouge = hf_evaluate.load('rouge')\n",
        "\n",
        "def generate_answers(questions, max_new_tokens=64):\n",
        "    inputs = tokenizer(['answer the question: ' + q for q in questions], return_tensors='tf', padding=True, truncation=True, max_length=MAX_SOURCE_LENGTH)\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "sample_size = min(256, len(val_df))\n",
        "sample_questions = val_df[QUESTION_COL].tolist()[:sample_size]\n",
        "sample_refs = [[a] for a in val_df[ANSWER_COL].tolist()[:sample_size]]\n",
        "\n",
        "preds = generate_answers(sample_questions)\n",
        "\n",
        "bleu_res = bleu.compute(predictions=preds, references=sample_refs)\n",
        "rouge_res = rouge.compute(predictions=preds, references=[r[0] for r in sample_refs])\n",
        "\n",
        "val_loss = model.evaluate(val_ds, return_dict=True)['loss']\n",
        "perplexity = math.exp(val_loss) if val_loss < 20 else float('inf')\n",
        "\n",
        "print('BLEU:', bleu_res)\n",
        "print('ROUGE-L:', rouge_res.get('rougeL'))\n",
        "print('Val loss:', val_loss, 'Perplexity:', perplexity)\n"
      ],
      "metadata": {
        "id": "lBP3Q3Nsi3sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Qualitative test: predictions vs references on a small sample\n",
        "\n",
        "NUM_EXAMPLES = 10\n",
        "MAX_NEW_TOKENS = 64\n",
        "\n",
        "sample_questions = val_df[QUESTION_COL].tolist()[:NUM_EXAMPLES]\n",
        "sample_refs = val_df[ANSWER_COL].tolist()[:NUM_EXAMPLES]\n",
        "\n",
        "def generate_answers_list(questions, max_new_tokens=64):\n",
        "    inputs = tokenizer(\n",
        "        ['answer the question: ' + q for q in questions],\n",
        "        return_tensors='tf',\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SOURCE_LENGTH,\n",
        "    )\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "preds = generate_answers_list(sample_questions, max_new_tokens=MAX_NEW_TOKENS)\n",
        "\n",
        "for i, (q, ref, pred) in enumerate(zip(sample_questions, sample_refs, preds), 1):\n",
        "    print(f'--- Example {i} ---')\n",
        "    print('Q:', q)\n",
        "    print('REF:', ref[:400])\n",
        "    print('PRED:', pred[:400])\n",
        "    print()\n",
        "\n",
        "print(f'Shown {NUM_EXAMPLES} qualitative examples.')\n"
      ],
      "metadata": {
        "id": "R7wSQikdGYAs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}